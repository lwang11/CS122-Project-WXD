{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "\n",
    "import math\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import csv\n",
    "import pickle\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "import operator\n",
    "from collections import OrderedDict\n",
    "\n",
    "def read_and_preprocessing(json_filename, num_attribute, is_lower_case,\n",
    "    is_stem,is_remove_stopwords, is_remove_puctuation, stemmer, customized_stopwords):\n",
    "    data = json.load(open(json_filename))\n",
    "    documents = []\n",
    "    index_in_json = []\n",
    "    title_set = set()\n",
    "    word_set = set()\n",
    "    cnt = 0\n",
    "    len_data = str(len(data))\n",
    "    cur_customized_stopwords = set()\n",
    "\n",
    "    for i in range(0, len(data)):\n",
    "        if len(data[i]) == num_attribute and len(data[i]['ingredients']) != 0:\n",
    "            if data[i]['title'] not in title_set:\n",
    "                #print(str(i) + '/' + len_data)\n",
    "                title_set.add(data[i]['title'])\n",
    "                index_in_json.append(i)\n",
    "                ingredients = data[i]['ingredients']\n",
    "\n",
    "                actual_ingredients = []\n",
    "                for each_ingredient in ingredients:\n",
    "                    if is_lower_case:\n",
    "                        each_ingredient = each_ingredient.lower()\n",
    "\n",
    "                    tokens = nltk.word_tokenize(each_ingredient)\n",
    "\n",
    "                    if is_stem:\n",
    "                        singles = [stemmer.stem(token) for token in tokens]\n",
    "                        for ele in customized_stopwords:\n",
    "                            cur_customized_stopwords.add(stemmer.stem(ele))\n",
    "\n",
    "                    if is_remove_stopwords:\n",
    "                        filtered_words = [word for word in singles if (word\n",
    "                            not in stopwords.words('english') and\n",
    "                            (word not in cur_customized_stopwords))]\n",
    "\n",
    "                    else:\n",
    "                        filtered_words = singles\n",
    "                    filtered_words_2 = []\n",
    "                    if is_remove_puctuation:\n",
    "                        for word in filtered_words:\n",
    "                            if word.isalpha():\n",
    "                                filtered_words_2.append(word)\n",
    "                                word_set.add(word)\n",
    "                        # filtered_words_2 = [word for word in filtered_words if word.isalpha()]\n",
    "                    else:\n",
    "                        filtered_words_2 = filtered_words\n",
    "                    # print('-----------------------------------')\n",
    "                    # print(filtered_words_2)\n",
    "                    actual_ingredients = actual_ingredients + filtered_words_2\n",
    "                documents.append(actual_ingredients)\n",
    "\n",
    "\n",
    "\n",
    "    return index_in_json, documents, word_set\n",
    "\n",
    "\n",
    "def generate_inverted_index(index_in_json, documents, word_set):\n",
    "\n",
    "    N = len(documents)\n",
    "    num_word = len(word_set)\n",
    "    cur_word = 0\n",
    "    inverted_index = {}\n",
    "    for each_word in word_set:\n",
    "        inverted_index[each_word] = [0]\n",
    "        cur_word = cur_word + 1\n",
    "        print(cur_word, num_word)\n",
    "\n",
    "        for i in range(0, len(documents)):\n",
    "            index = index_in_json[i]\n",
    "            document = documents[i]\n",
    "            # print(each_word)\n",
    "            # print(document)\n",
    "            if each_word in document:\n",
    "                inverted_index[each_word][0] = inverted_index[each_word][0] + 1\n",
    "                # inverted_index[each_word].append((index, document.count(each_word)))\n",
    "                inverted_index[each_word].append((i, document.count(each_word)))\n",
    "        inverted_index[each_word][0] = 1.0 + math.log(float(N) / float(inverted_index[each_word][0]))\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "def get_document_length(index_in_json, documents, inverted_index):\n",
    "    doc_length = []\n",
    "    for i in range(0, len(documents)):\n",
    "        document = documents[i]\n",
    "        length = 0.0\n",
    "        print(i,len(documents))\n",
    "        for each_word in document:\n",
    "            tf = math.log(document.count(each_word) + 1.0)\n",
    "            idf = inverted_index[each_word][0]\n",
    "            length = length + tf*idf*tf*idf\n",
    "        doc_length.append(math.sqrt(length))\n",
    "    return doc_length\n",
    "\n",
    "\n",
    "def doc_ranking(query, index_in_json, documents, inverted_index, doc_length):\n",
    "    #preprocessing query\n",
    "    query_set = set()\n",
    "    if is_lower_case:\n",
    "        query = query.lower()\n",
    "    tokens = nltk.word_tokenize(query)\n",
    "    if is_stem:\n",
    "        singles = [stemmer.stem(token) for token in tokens]\n",
    "    if is_remove_stopwords:\n",
    "        filtered_words = [word for word in singles if word not in stopwords.words('english')]\n",
    "    else:\n",
    "        filtered_words = singles\n",
    "    filtered_words_2 = []\n",
    "    if is_remove_puctuation:\n",
    "        for word in filtered_words:\n",
    "            if word.isalpha():\n",
    "                filtered_words_2.append(word)\n",
    "                query_set.add(word)\n",
    "    else:\n",
    "        filtered_words_2 = filtered_words\n",
    "    #print(filtered_words_2)\n",
    "    #doc doc_ranking\n",
    "    doc_rank = {}\n",
    "    query_length = 0.0\n",
    "    for each_word in query_set:\n",
    "        if each_word in inverted_index:\n",
    "            tf_query = math.log(filtered_words_2.count(each_word) + 1.0)\n",
    "            idf_query = inverted_index[each_word][0]\n",
    "            query_word_weight = tf_query * idf_query\n",
    "            query_length = query_length + query_word_weight*query_word_weight\n",
    "            for i in range(1, len(inverted_index[each_word])):\n",
    "                document_index = inverted_index[each_word][i][0]\n",
    "                document_count = inverted_index[each_word][i][1]\n",
    "                tf_doc = math.log(document_count + 1.0)\n",
    "                idf_doc = inverted_index[each_word][0]\n",
    "                doc_word_weight = tf_doc * idf_doc\n",
    "                if document_index not in doc_rank:\n",
    "                    doc_rank[document_index] = 0.0\n",
    "                doc_rank[document_index] = doc_rank[document_index] + doc_word_weight * query_word_weight\n",
    "\n",
    "    query_length = math.sqrt(query_length)\n",
    "    for document_index in doc_rank:\n",
    "        doc_rank[document_index] = float(doc_rank[document_index]) / (query_length * doc_length[document_index])\n",
    "    return doc_rank\n",
    "\n",
    "def delete_food(sorted_doc_rank, documents, without_food):\n",
    "    if len(without_food) == 0:\n",
    "        filtered_doc_index = []\n",
    "        for ele in sorted_doc_rank:\n",
    "            index = ele[0]\n",
    "            filtered_doc_index.append(index)\n",
    "    else:\n",
    "        without_food_set = set()\n",
    "        if is_lower_case:\n",
    "            without_food = without_food.lower()\n",
    "        tokens = nltk.word_tokenize(without_food)\n",
    "        if is_stem:\n",
    "            singles = [stemmer.stem(token) for token in tokens]\n",
    "        if is_remove_stopwords:\n",
    "            filtered_words = [word for word in singles if word not in stopwords.words('english')]\n",
    "        else:\n",
    "            filtered_words = singles\n",
    "        filtered_words_2 = []\n",
    "        if is_remove_puctuation:\n",
    "            for word in filtered_words:\n",
    "                if word.isalpha():\n",
    "                    filtered_words_2.append(word)\n",
    "                    without_food_set.add(word)\n",
    "        else:\n",
    "            filtered_words_2 = filtered_words\n",
    "        filtered_doc_index = []\n",
    "        for ele in sorted_doc_rank:\n",
    "            index = ele[0]\n",
    "            is_valid = True\n",
    "            for each_no_word in without_food_set:\n",
    "                if each_no_word in documents[index]:\n",
    "                    is_valid = False\n",
    "                    break\n",
    "            if is_valid:\n",
    "                filtered_doc_index.append(index)\n",
    "    return filtered_doc_index\n",
    "\n",
    "\n",
    "\n",
    "def get_data(json_filename, top_n, doc_index, documents):\n",
    "    data = json.load(open(json_filename))\n",
    "    to_print ={}\n",
    "    for i in range(0, top_n):\n",
    "        index = load_index_in_json[doc_index[i]]\n",
    "        to_print['title'] = data[index]['title']\n",
    "        to_print['ingredients'] = documents[doc_index[i]]\n",
    "        # data[index]['ingredients']\n",
    "\n",
    "        print('title: ' + data[index]['title'] + ' ' + str(len(documents[doc_index[i]])))\n",
    "        print(documents[doc_index[i]])\n",
    "        print('------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "def save_func(filename, data):\n",
    "    # print('start saving ' + filename)\n",
    "    with open(filename, \"wb\") as fp1:\n",
    "        pickle.dump(data, fp1)\n",
    "    # print('end saving ' + filename)\n",
    "\n",
    "def load_func(filename):\n",
    "    # print('start loading ' + filename)\n",
    "    with open(filename, \"rb\") as fp1:\n",
    "        data = pickle.load(fp1)\n",
    "    # print('end loading ' + filename)\n",
    "    return data\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "is_lower_case = True\n",
    "is_stem = True\n",
    "is_remove_stopwords = True\n",
    "is_remove_puctuation = True\n",
    "\n",
    "name_documents = 'documents'\n",
    "name_index_in_json = 'index_in_json'\n",
    "name_word_set = 'word_set'\n",
    "name_inverted_index = 'inverted_index'\n",
    "num_attribute = 11\n",
    "json_filename = 'full_format_recipes.json'\n",
    "name_doc_length = 'doc_length'\n",
    "\n",
    "customized_stopwords = {\"spoon\", \"cups\", \"large\",\n",
    "    \"teaspoon\", \"medium\", \"small\", \"Freshly\", \"sheets\", \"pound\",\n",
    "    \"tablespoon\", \"ounce\", \"lb\"}\n",
    "\n",
    "index_in_json, documents, word_set = read_and_preprocessing(json_filename, num_attribute, is_lower_case, is_stem,is_remove_stopwords, is_remove_puctuation, stemmer, customized_stopwords)\n",
    "\n",
    "save_func(name_documents, documents)\n",
    "save_func(name_index_in_json, index_in_json)\n",
    "save_func(name_word_set, word_set)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "load_documents = load_func(name_documents)\n",
    "load_index_in_json = load_func(name_index_in_json)\n",
    "load_word_set = load_func(name_word_set)\n",
    "#\n",
    "#\n",
    "inverted_index = generate_inverted_index(load_index_in_json, load_documents, load_word_set)\n",
    "save_func(name_inverted_index, inverted_index)\n",
    "load_inverted_index = load_func(name_inverted_index)\n",
    "#\n",
    "doc_length = get_document_length(load_index_in_json, load_documents, load_inverted_index)\n",
    "save_func(name_doc_length, doc_length)\n",
    "load_doc_length = load_func(name_doc_length)\n",
    "\n",
    "def find_recipe(json_filename, query, top_n, without_food, load_index_in_json, load_documents, load_inverted_index, load_doc_length):\n",
    "    doc_rank = doc_ranking(query, load_index_in_json, load_documents, load_inverted_index, load_doc_length)\n",
    "    sorted_doc_rank = sorted(doc_rank.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    data = json.load(open(json_filename))\n",
    "    for i in range(0, 5):\n",
    "        index = load_index_in_json[sorted_doc_rank[i][0]]\n",
    "    \n",
    "    filtered_doc_index = delete_food(sorted_doc_rank, load_documents, without_food)    \n",
    "    data = get_data(json_filename, top_n, filtered_doc_index, load_documents)\n",
    "    return data\n",
    "        \n",
    "#\n",
    "query = 'potato, beef'\n",
    "#\n",
    "    \n",
    "# # pprint(sorted_doc_rank[0:8])\n",
    "    \n",
    "# #     pprint(data[index])\n",
    "#\n",
    "# # without_food = 'olive oil'\n",
    "# # without_food = ''\n",
    "without_food = 'mushroom'\n",
    "        #filtered_doc_index = delete_food(sorted_doc_rank, load_documents, without_food)\n",
    "# # print(filtered_doc_index[0:5])\n",
    "top_n = 5\n",
    "#\n",
    "# #\n",
    "find_recipe(json_filename, query, top_n, without_food, load_index_in_json, load_documents, load_inverted_index, load_doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
